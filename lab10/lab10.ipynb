{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Arturo Adelfio (s316716)\n",
    "\n",
    "`not completed due to time constraints`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1 #first player win\n",
    "    elif win(pos.o):\n",
    "        return -1 #second player win\n",
    "    else:\n",
    "        return 0 #drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "        \n",
    "    def move(self,possible_moves, state=None):\n",
    "        return choice(list(possible_moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_prob=1.0, exploration_decay=0.995):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_prob = exploration_prob\n",
    "        self.exploration_decay = exploration_decay\n",
    "        # Initialize Q-table \n",
    "        self.q_table = {}\n",
    "        self.training = True\n",
    "\n",
    "    def move(self, possible_moves, state):\n",
    "        # Exploration-exploitation trade-off\n",
    "        #print(self.q_table)\n",
    "        if np.random.rand() < self.exploration_prob: #Explore\n",
    "            return choice(list(possible_moves))\n",
    "        else: #Exploit\n",
    "            #Choosing from the QTable\n",
    "            action = np.argmax(self.q_table[state, :]) \n",
    "        \n",
    "        \n",
    "            \n",
    "        return action\n",
    "        \n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        # Convert next_state to frozenset if it's a set\n",
    "        print(\"Next state\", next_state)\n",
    "        next_state = (frozenset(next_state.x), frozenset(next_state.o))\n",
    "        state = (frozenset(state.x), frozenset(state.o))\n",
    "\n",
    "        # Q-value update using the Bellman equation\n",
    "        print(self.q_table)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros((10,))\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros((10,))\n",
    "        \n",
    "        current_q_value = self.q_table[state][action]\n",
    "        max_future_q_value = np.max(self.q_table[next_state])\n",
    "        new_q_value = (1 - self.learning_rate) * current_q_value + \\\n",
    "                    self.learning_rate * (reward + self.discount_factor * max_future_q_value)\n",
    "        self.q_table[state][action] = new_q_value\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game(p1,p2, train = True):\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    \n",
    "    \n",
    "    game_hist = list()\n",
    "    players = []\n",
    "    moves=list()\n",
    "    players.append(p1)\n",
    "    players.append(p2)\n",
    "    index = 0\n",
    "    i = 0\n",
    "    current_player = players[index]\n",
    "    agent = None\n",
    "    \n",
    "    while True:\n",
    "        #print(current_player, index)\n",
    "        \n",
    "        if index == 0:\n",
    "            move = current_player.move(available, state)\n",
    "           \n",
    "            #print(available)\n",
    "            #print(move)\n",
    "            state.x.add(move)\n",
    "            \n",
    "            moves.append(move)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(move)\n",
    "            \n",
    "            if win(state.x) or not available:\n",
    "                break\n",
    "        else:\n",
    "            move = current_player.move(available, state)\n",
    "            state.o.add(move)\n",
    "            moves.append(move)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(move)\n",
    "            #print(available)\n",
    "            #print(move)\n",
    "            if win(state.o) or not available:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        isAgent = isinstance(current_player, QAgent)\n",
    "        \n",
    "        if isAgent and agent == None:\n",
    "            agent = current_player\n",
    "            previous_action = move\n",
    "        \n",
    "        if not isAgent  and i > 2: #QAgent opponent is playing, but it's not the first move, so QAgent already played\n",
    "            #print(trajectory)\n",
    "            previous_state = trajectory[-2]\n",
    "            #print(\"previous state\",previous_state)\n",
    "            #print(\"reward\", reward)\n",
    "            #print(\"last_move\", moves[-2])\n",
    "            reward = moves[-2] // 3\n",
    "                        \n",
    "            agent.update_q_table(previous_state, previous_action, reward, state)\n",
    "        \n",
    "        i = i+1\n",
    "            \n",
    "        \n",
    "            \n",
    "        index = 1 - index\n",
    "        current_player = players[index]\n",
    "        result = state_value(state)\n",
    "        #print(\"result\", result) #1 first winning, -1 second winning, 0 drawn\n",
    "        \n",
    "    return trajectory, moves, result, available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    player1 = RandomPlayer()\n",
    "    player0 = QAgent()\n",
    "    qAgent_wins = 0\n",
    "    qAgent_lost = 0\n",
    "    games_number = 100\n",
    "    \n",
    "    for _ in tqdm(range(games_number)):\n",
    "        trajectory, moves, reward, possible_moves = game(player0, player1, train=True)\n",
    "        \n",
    "        if reward == 1: #first player (QAgent wins)\n",
    "            qAgent_wins += 1\n",
    "            reward = reward + 10\n",
    "            state = trajectory[-1]\n",
    "            move = moves[-1]\n",
    "            #new_state = deepcopy(state)\n",
    "            #new_state.x.add(choice(possible_moves))\n",
    "        elif reward == -1:\n",
    "            reward == reward -20\n",
    "            qAgent_lost +=1\n",
    "            state = trajectory[-2]\n",
    "            move = moves[-2]\n",
    "            new_state = trajectory[-1]\n",
    "        else:\n",
    "            state = trajectory[-1]\n",
    "            move = moves[-1]\n",
    "            #new_state = deepcopy(state)\n",
    "            #new_state.x.add(choice(possible_moves))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        new_state = trajectory[-1]\n",
    "        \n",
    "            \n",
    "        player0.update_q_table(state, move, reward, new_state)\n",
    "        \n",
    "    \n",
    "     \n",
    "    print(\"Winning/Draw\", (games_number - qAgent_lost)/games_number)\n",
    "    print(\"Lost games\", qAgent_lost/games_number)   \n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
